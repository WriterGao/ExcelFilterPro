"""
æ•°æ®æ˜ å°„å¼•æ“
æ”¯æŒ"æŸ¥æ‰¾åŒ¹é…å¹¶å¤åˆ¶æ•°æ®"çš„æ“ä½œ
"""

import pandas as pd
from typing import List, Dict, Any, Tuple, Optional
from ..utils.logger import get_logger
from ..database.models import DataMapping, FilterOperator, ExcelCoordinate


class DataMappingEngine:
    """æ•°æ®æ˜ å°„å¼•æ“"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        self.logger.info("ğŸ“‹ æ•°æ®æ˜ å°„å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def execute_mapping(self, mapping: DataMapping, source_data: Dict[str, pd.DataFrame], 
                       target_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """
        æ‰§è¡Œæ•°æ®æ˜ å°„
        
        Args:
            mapping: æ•°æ®æ˜ å°„é…ç½®
            source_data: æºæ•°æ® {æ–‡ä»¶å: DataFrame}
            target_data: ç›®æ ‡æ•°æ® {æ–‡ä»¶å: DataFrame}
            
        Returns:
            æ›´æ–°åçš„ç›®æ ‡æ•°æ®
        """
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®æ˜ å°„: {mapping.name}")
            self.logger.info(f"ğŸ“ æ˜ å°„æè¿°: {mapping.description}")
            
            # æ‰“å°æ˜ å°„é…ç½®è¯¦æƒ…
            self.logger.info(f"ğŸ”§ æ˜ å°„é…ç½®è¯¦æƒ…:")
            self.logger.info(f"   æºæ–‡ä»¶: {mapping.source_file}")
            self.logger.info(f"   æºåŒ¹é…åˆ—: {mapping.source_match_coordinate}")
            self.logger.info(f"   æºåŒ¹é…å€¼: '{mapping.source_match_value}'")
            self.logger.info(f"   æºå–å€¼åˆ—: {mapping.source_value_coordinate}")
            self.logger.info(f"   ç›®æ ‡æ–‡ä»¶: {mapping.target_file}")
            self.logger.info(f"   ç›®æ ‡åŒ¹é…åˆ—: {mapping.target_match_coordinate}")
            self.logger.info(f"   ç›®æ ‡åŒ¹é…å€¼: '{mapping.target_match_value}'")
            self.logger.info(f"   ç›®æ ‡æ’å…¥åˆ—: {mapping.target_insert_coordinate}")
            self.logger.info(f"   åŒ¹é…æ“ä½œç¬¦: {mapping.match_operator.value}")
            
            # æ‰“å°æºæ•°æ®è¯¦æƒ…
            if mapping.source_file in source_data:
                source_df = source_data[mapping.source_file]
                self.logger.info(f"ğŸ“Š æºæ•°æ®æ–‡ä»¶è¯¦æƒ…:")
                self.logger.info(f"   æ–‡ä»¶å: {mapping.source_file}")
                self.logger.info(f"   æ•°æ®å½¢çŠ¶: {source_df.shape} (è¡Œæ•°: {source_df.shape[0]}, åˆ—æ•°: {source_df.shape[1]})")
                
                # æ˜¾ç¤ºæºæ•°æ®çš„å‰å‡ è¡Œ
                self.logger.info(f"   æºæ•°æ®å‰5è¡Œ:")
                for idx, row in source_df.head(5).iterrows():
                    row_data = [str(val)[:20] + "..." if len(str(val)) > 20 else str(val) for val in row.values]
                    self.logger.info(f"     ç¬¬{idx+1}è¡Œ: {row_data}")
            
            # æ‰“å°ç›®æ ‡æ•°æ®è¯¦æƒ…
            if mapping.target_file in target_data:
                target_df = target_data[mapping.target_file]
                self.logger.info(f"ğŸ¯ ç›®æ ‡æ•°æ®æ–‡ä»¶è¯¦æƒ…:")
                self.logger.info(f"   æ–‡ä»¶å: {mapping.target_file}")
                self.logger.info(f"   æ•°æ®å½¢çŠ¶: {target_df.shape} (è¡Œæ•°: {target_df.shape[0]}, åˆ—æ•°: {target_df.shape[1]})")
                
                # æ˜¾ç¤ºç›®æ ‡æ•°æ®çš„å‰å‡ è¡Œ
                self.logger.info(f"   ç›®æ ‡æ•°æ®å‰5è¡Œ:")
                for idx, row in target_df.head(5).iterrows():
                    row_data = [str(val)[:20] + "..." if len(str(val)) > 20 else str(val) for val in row.values]
                    self.logger.info(f"     ç¬¬{idx+1}è¡Œ: {row_data}")
            
            # 1. ä»æºæ–‡ä»¶ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ•°æ®å¹¶æå–å€¼
            source_values = self._extract_source_values(mapping, source_data)
            if not source_values:
                self.logger.warning(f"âŒ æºæ–‡ä»¶ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®: {mapping.source_match_value}")
                return target_data
            
            # 2. åœ¨ç›®æ ‡æ–‡ä»¶ä¸­æ‰¾åˆ°åŒ¹é…çš„ä½ç½®
            target_positions = self._find_target_positions(mapping, target_data)
            if not target_positions:
                self.logger.info(f"âŒ ç›®æ ‡æ–‡ä»¶ {mapping.target_file} ä¸­æœªæ‰¾åˆ°åŒ¹é…å€¼ '{mapping.target_match_value}' çš„ä½ç½®ï¼Œå°†è·³è¿‡æ­¤æ˜ å°„")
                # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£ç›®æ ‡æ–‡ä»¶çš„æ•°æ®ç»“æ„
                if mapping.target_file in target_data:
                    df = target_data[mapping.target_file]
                    match_col_index = self._get_column_index(mapping.target_match_coordinate, df)
                    if match_col_index < len(df.columns):
                        unique_values = df.iloc[:, match_col_index].dropna().unique()[:10]  # åªæ˜¾ç¤ºå‰10ä¸ªå”¯ä¸€å€¼
                        self.logger.info(f"   ç›®æ ‡åŒ¹é…åˆ—çš„å¯ç”¨å€¼ç¤ºä¾‹: {list(unique_values)}")
                return target_data
            
            # 3. å°†æºå€¼æ’å…¥åˆ°ç›®æ ‡ä½ç½®
            updated_target_data = self._insert_values(mapping, target_data, source_values, target_positions)
            
            self.logger.info(f"âœ… æ•°æ®æ˜ å°„å®Œæˆ: å¤„ç†äº† {len(source_values)} ä¸ªæºå€¼ï¼Œ{len(target_positions)} ä¸ªç›®æ ‡ä½ç½®")
            return updated_target_data
            
        except Exception as e:
            self.logger.error(f"âŒ æ‰§è¡Œæ•°æ®æ˜ å°„å¤±è´¥: {e}")
            raise
    
    def _extract_source_values(self, mapping: DataMapping, source_data: Dict[str, pd.DataFrame]) -> List[Any]:
        """ä»æºæ–‡ä»¶ä¸­æå–åŒ¹é…çš„å€¼"""
        if mapping.source_file not in source_data:
            raise ValueError(f"æºæ–‡ä»¶æœªæ‰¾åˆ°: {mapping.source_file}")
        
        df = source_data[mapping.source_file]
        self.logger.info(f"ğŸ” å¼€å§‹ä»æºæ–‡ä»¶æå–æ•°æ®:")
        
        # è·å–æºåŒ¹é…åˆ—å’Œå–å€¼åˆ—çš„ç´¢å¼•
        match_col_index = self._get_column_index(mapping.source_match_coordinate, df)
        value_col_index = self._get_column_index(mapping.source_value_coordinate, df)
        
        self.logger.info(f"   æºåŒ¹é…åˆ—ç´¢å¼•: {match_col_index} ({mapping.source_match_coordinate})")
        self.logger.info(f"   æºå–å€¼åˆ—ç´¢å¼•: {value_col_index} ({mapping.source_value_coordinate})")
        
        if match_col_index >= len(df.columns) or value_col_index >= len(df.columns):
            raise ValueError("åˆ—ç´¢å¼•è¶…å‡ºèŒƒå›´")
        
        # æŸ¥æ‰¾åŒ¹é…çš„è¡Œ
        match_column = df.iloc[:, match_col_index]
        value_column = df.iloc[:, value_col_index]
        
        # æ–°å¢ï¼šåªåœ¨æŒ‡å®šè¡ŒèŒƒå›´æŸ¥æ‰¾
        if mapping.source_match_row_range:
            start, end = mapping.source_match_row_range
            # Excelè¡Œå·ä»1å¼€å§‹ï¼ŒDataFrameç´¢å¼•ä»0å¼€å§‹
            match_column = match_column.iloc[start-1:end]
            value_column = value_column.iloc[start-1:end]
            self.logger.info(f"   ä»…åœ¨ç¬¬{start}è¡Œåˆ°ç¬¬{end}è¡ŒæŸ¥æ‰¾åŒ¹é…")
        
        # æ‰“å°åŒ¹é…åˆ—çš„æ‰€æœ‰å€¼
        self.logger.info(f"   æºåŒ¹é…åˆ—(ç¬¬{match_col_index+1}åˆ—)çš„æ‰€æœ‰å€¼:")
        for idx, val in match_column.items():
            val_str = str(val) if pd.notna(val) else "ç©ºå€¼"
            if isinstance(idx, int):
                row_no = idx + 1
            else:
                row_no = idx
            self.logger.info(f"     ç¬¬{row_no}è¡Œ: '{val_str}'")
        
        self.logger.info(f"   æ­£åœ¨æŸ¥æ‰¾åŒ¹é…å€¼: '{mapping.source_match_value}' (æ“ä½œç¬¦: {mapping.match_operator.value})")
        
        # æ ¹æ®æ“ä½œç¬¦è¿›è¡ŒåŒ¹é…
        matched_rows = self._apply_match_operator(
            match_column, mapping.source_match_value, mapping.match_operator
        )
        
        # æ˜¾ç¤ºåŒ¹é…ç»“æœ
        matched_indices = matched_rows[matched_rows].index.tolist()
        self.logger.info(f"   æ‰¾åˆ°åŒ¹é…çš„è¡Œç´¢å¼•: {matched_indices}")
        
        if matched_indices:
            self.logger.info(f"   åŒ¹é…è¡Œçš„è¯¦ç»†ä¿¡æ¯:")
            for idx in matched_indices:
                match_val = match_column.loc[idx]
                value_val = value_column.loc[idx]
                if isinstance(idx, int):
                    row_no = idx + 1
                else:
                    row_no = idx
                self.logger.info(f"     ç¬¬{row_no}è¡Œ: åŒ¹é…åˆ—='{match_val}', å–å€¼åˆ—='{value_val}'")
        
        # æå–åŒ¹é…è¡Œçš„å€¼
        source_values = value_column[matched_rows].dropna().tolist()
        
        self.logger.info(f"âœ… ä»æºæ–‡ä»¶æå–åˆ° {len(source_values)} ä¸ªå€¼: {source_values}")
        return source_values
    
    def _find_target_positions(self, mapping: DataMapping, target_data: Dict[str, pd.DataFrame]) -> List[int]:
        """åœ¨ç›®æ ‡æ–‡ä»¶ä¸­æŸ¥æ‰¾åŒ¹é…çš„ä½ç½®"""
        if mapping.target_file not in target_data:
            raise ValueError(f"ç›®æ ‡æ–‡ä»¶æœªæ‰¾åˆ°: {mapping.target_file}")
        
        df = target_data[mapping.target_file]
        self.logger.info(f"ğŸ¯ å¼€å§‹åœ¨ç›®æ ‡æ–‡ä»¶ä¸­æŸ¥æ‰¾æ’å…¥ä½ç½®:")
        
        # è·å–ç›®æ ‡åŒ¹é…åˆ—çš„ç´¢å¼•
        match_col_index = self._get_column_index(mapping.target_match_coordinate, df)
        
        self.logger.info(f"   ç›®æ ‡åŒ¹é…åˆ—ç´¢å¼•: {match_col_index} ({mapping.target_match_coordinate})")
        
        if match_col_index >= len(df.columns):
            raise ValueError("ç›®æ ‡åŒ¹é…åˆ—ç´¢å¼•è¶…å‡ºèŒƒå›´")
        
        # æŸ¥æ‰¾åŒ¹é…çš„è¡Œ
        match_column = df.iloc[:, match_col_index]
        
        # æ–°å¢ï¼šåªåœ¨æŒ‡å®šè¡ŒèŒƒå›´æŸ¥æ‰¾
        if mapping.target_match_row_range:
            start, end = mapping.target_match_row_range
            match_column = match_column.iloc[start-1:end]
            self.logger.info(f"   ä»…åœ¨ç¬¬{start}è¡Œåˆ°ç¬¬{end}è¡ŒæŸ¥æ‰¾åŒ¹é…")
        
        # æ‰“å°ç›®æ ‡åŒ¹é…åˆ—çš„æ‰€æœ‰å€¼
        self.logger.info(f"   ç›®æ ‡åŒ¹é…åˆ—(ç¬¬{match_col_index+1}åˆ—)çš„æ‰€æœ‰å€¼:")
        for idx, val in match_column.items():
            val_str = str(val) if pd.notna(val) else "ç©ºå€¼"
            if isinstance(idx, int):
                row_no = idx + 1
            else:
                row_no = idx
            self.logger.info(f"     ç¬¬{row_no}è¡Œ: '{val_str}'")
        
        self.logger.info(f"   æ­£åœ¨æŸ¥æ‰¾ç›®æ ‡åŒ¹é…å€¼: '{mapping.target_match_value}'")
        
        # æ ¹æ®æ“ä½œç¬¦è¿›è¡ŒåŒ¹é…ï¼ˆç›®æ ‡åŒ¹é…é€šå¸¸ä½¿ç”¨ç­‰äºæ“ä½œç¬¦ï¼‰
        matched_rows = self._apply_match_operator(
            match_column, mapping.target_match_value, mapping.match_operator
        )
        
        # è·å–åŒ¹é…è¡Œçš„ç´¢å¼•
        target_positions = matched_rows[matched_rows].index.tolist()
        
        self.logger.info(f"   æ‰¾åˆ°ç›®æ ‡åŒ¹é…è¡Œç´¢å¼•: {target_positions}")
        
        if target_positions:
            self.logger.info(f"   ç›®æ ‡åŒ¹é…è¡Œçš„è¯¦ç»†ä¿¡æ¯:")
            for pos in target_positions:
                match_val = match_column.loc[pos]
                if isinstance(pos, int):
                    row_no = pos + 1
                else:
                    row_no = pos
                self.logger.info(f"     ç¬¬{row_no}è¡Œ: åŒ¹é…åˆ—='{match_val}'")
        
        self.logger.info(f"âœ… åœ¨ç›®æ ‡æ–‡ä»¶æ‰¾åˆ° {len(target_positions)} ä¸ªåŒ¹é…ä½ç½®")
        return target_positions
    
    def _insert_values(self, mapping: DataMapping, target_data: Dict[str, pd.DataFrame], 
                      source_values: List[Any], target_positions: List[int]) -> Dict[str, pd.DataFrame]:
        """å°†æºå€¼æ’å…¥åˆ°ç›®æ ‡ä½ç½®"""
        updated_target_data = target_data.copy()
        df = updated_target_data[mapping.target_file].copy()
        
        self.logger.info(f"ğŸ“ å¼€å§‹æ’å…¥æ•°æ®åˆ°ç›®æ ‡æ–‡ä»¶:")
        
        # è·å–ç›®æ ‡æ’å…¥åˆ—çš„ç´¢å¼•
        insert_col_index = self._get_column_index(mapping.target_insert_coordinate, df)
        
        self.logger.info(f"   ç›®æ ‡æ’å…¥åˆ—ç´¢å¼•: {insert_col_index} ({mapping.target_insert_coordinate})")
        
        if insert_col_index >= len(df.columns):
            raise ValueError("ç›®æ ‡æ’å…¥åˆ—ç´¢å¼•è¶…å‡ºèŒƒå›´")
        
        # æ˜¾ç¤ºæ’å…¥å‰çš„ç›®æ ‡åˆ—çŠ¶æ€
        insert_column = df.iloc[:, insert_col_index]
        self.logger.info(f"   æ’å…¥å‰ç›®æ ‡åˆ—(ç¬¬{insert_col_index+1}åˆ—)çš„å€¼:")
        for idx, val in insert_column.items():
            val_str = str(val) if pd.notna(val) else "ç©ºå€¼"
            if isinstance(idx, int):
                row_no = idx + 1
            else:
                row_no = idx
            self.logger.info(f"     ç¬¬{row_no}è¡Œ: '{val_str}'")
        
        # æ’å…¥æ•°æ®
        insert_count = 0
        self.logger.info(f"   å‡†å¤‡æ’å…¥çš„æºå€¼: {source_values}")
        self.logger.info(f"   ç›®æ ‡æ’å…¥ä½ç½®: {[pos+1 for pos in target_positions]} (è¡Œå·)")
        
        for i, pos in enumerate(target_positions):
            if i < len(source_values):
                old_value = df.iloc[pos, insert_col_index]
                new_value = source_values[i]
                
                # æ£€æŸ¥æ˜¯å¦å…è®¸è¦†ç›–
                if mapping.overwrite_existing or pd.isna(old_value):
                    df.iloc[pos, insert_col_index] = new_value
                    insert_count += 1
                    self.logger.info(f"   âœ… ç¬¬{pos+1}è¡Œ: '{old_value}' â†’ '{new_value}'")
                else:
                    self.logger.info(f"   â­ï¸  ç¬¬{pos+1}è¡Œ: è·³è¿‡è¦†ç›– '{old_value}' (overwrite_existing=False)")
            else:
                # æºå€¼æ•°é‡ä¸è¶³ï¼Œå¯ä»¥é‡å¤ä½¿ç”¨æœ€åä¸€ä¸ªå€¼æˆ–è€…åœæ­¢
                if source_values:  # å¦‚æœæœ‰å€¼ï¼Œä½¿ç”¨æœ€åä¸€ä¸ª
                    old_value = df.iloc[pos, insert_col_index]
                    new_value = source_values[-1]
                    
                    if mapping.overwrite_existing or pd.isna(old_value):
                        df.iloc[pos, insert_col_index] = new_value
                        insert_count += 1
                        self.logger.info(f"   âœ… ç¬¬{pos+1}è¡Œ: '{old_value}' â†’ '{new_value}' (é‡å¤ä½¿ç”¨æœ€åå€¼)")
                    else:
                        self.logger.info(f"   â­ï¸  ç¬¬{pos+1}è¡Œ: è·³è¿‡è¦†ç›– '{old_value}' (overwrite_existing=False)")
        
        updated_target_data[mapping.target_file] = df
        
        # æ˜¾ç¤ºæ’å…¥åçš„ç›®æ ‡åˆ—çŠ¶æ€
        updated_insert_column = df.iloc[:, insert_col_index]
        self.logger.info(f"   æ’å…¥åç›®æ ‡åˆ—(ç¬¬{insert_col_index+1}åˆ—)çš„å€¼:")
        for idx, val in updated_insert_column.items():
            val_str = str(val) if pd.notna(val) else "ç©ºå€¼"
            if isinstance(idx, int):
                row_no = idx + 1
            else:
                row_no = idx
            self.logger.info(f"     ç¬¬{row_no}è¡Œ: '{val_str}'")
        
        self.logger.info(f"âœ… æˆåŠŸæ’å…¥ {insert_count} ä¸ªå€¼åˆ°ç›®æ ‡æ–‡ä»¶")
        return updated_target_data
    
    def _get_column_index(self, coordinate: ExcelCoordinate, df: pd.DataFrame) -> int:
        """è·å–åæ ‡å¯¹åº”çš„åˆ—ç´¢å¼•"""
        if coordinate.coord_type == "column":
            # æ•´åˆ—ï¼šæ ¹æ®åˆ—æ ‡è¯†è·å–ç´¢å¼•
            return coordinate.column_to_index(coordinate.column)
        elif coordinate.coord_type == "single":
            # å•ä¸ªåæ ‡ï¼šæ ¹æ®åˆ—æ ‡è¯†è·å–ç´¢å¼•
            return coordinate.column_to_index(coordinate.column)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åæ ‡ç±»å‹ç”¨äºåˆ—ç´¢å¼•: {coordinate.coord_type}")
    
    def _apply_match_operator(self, column: pd.Series, value: Any, operator: FilterOperator) -> pd.Series:
        """åº”ç”¨åŒ¹é…æ“ä½œç¬¦"""
        try:
            if operator == FilterOperator.EQUALS:
                # å…ˆå°è¯•ç›´æ¥åŒ¹é…
                direct_match = column == value
                
                # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œå°è¯•ç±»å‹è½¬æ¢åŒ¹é…
                if not direct_match.any():
                    # å°è¯•å°†ç›®æ ‡å€¼è½¬æ¢ä¸ºä¸åŒç±»å‹è¿›è¡ŒåŒ¹é…
                    type_converted_matches = []
                    
                    # å¦‚æœvalueæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è½¬æ¢ä¸ºæ•°å­—
                    if isinstance(value, str):
                        try:
                            # å°è¯•è½¬æ¢ä¸ºæ•´æ•°
                            int_value = int(value)
                            int_match = column == int_value
                            type_converted_matches.append(int_match)
                            self.logger.info(f"   ğŸ”„ å°è¯•æ•´æ•°åŒ¹é…: '{value}' -> {int_value}, åŒ¹é…åˆ°: {int_match.sum()}ä¸ª")
                        except ValueError:
                            pass
                        
                        try:
                            # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                            float_value = float(value)
                            float_match = column == float_value
                            type_converted_matches.append(float_match)
                            self.logger.info(f"   ğŸ”„ å°è¯•æµ®ç‚¹æ•°åŒ¹é…: '{value}' -> {float_value}, åŒ¹é…åˆ°: {float_match.sum()}ä¸ª")
                        except ValueError:
                            pass
                    
                    # å¦‚æœvalueæ˜¯æ•°å­—ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    elif isinstance(value, (int, float)):
                        str_value = str(value)
                        str_match = column.astype(str) == str_value
                        type_converted_matches.append(str_match)
                        self.logger.info(f"   ğŸ”„ å°è¯•å­—ç¬¦ä¸²åŒ¹é…: {value} -> '{str_value}', åŒ¹é…åˆ°: {str_match.sum()}ä¸ª")
                    
                    # åˆå¹¶æ‰€æœ‰åŒ¹é…ç»“æœ
                    if type_converted_matches:
                        combined_match = direct_match
                        for match in type_converted_matches:
                            combined_match = combined_match | match
                        return combined_match
                
                return direct_match
                
            elif operator == FilterOperator.NOT_EQUALS:
                return column != value
            elif operator == FilterOperator.CONTAINS:
                return column.astype(str).str.contains(str(value), na=False)
            elif operator == FilterOperator.NOT_CONTAINS:
                return ~column.astype(str).str.contains(str(value), na=False)
            elif operator == FilterOperator.STARTS_WITH:
                return column.astype(str).str.startswith(str(value), na=False)
            elif operator == FilterOperator.ENDS_WITH:
                return column.astype(str).str.endswith(str(value), na=False)
            elif operator == FilterOperator.GREATER_THAN:
                return pd.to_numeric(column, errors='coerce') > float(value)
            elif operator == FilterOperator.GREATER_EQUAL:
                return pd.to_numeric(column, errors='coerce') >= float(value)
            elif operator == FilterOperator.LESS_THAN:
                return pd.to_numeric(column, errors='coerce') < float(value)
            elif operator == FilterOperator.LESS_EQUAL:
                return pd.to_numeric(column, errors='coerce') <= float(value)
            elif operator == FilterOperator.IS_EMPTY:
                return column.isna() | (column.astype(str).str.strip() == '')
            elif operator == FilterOperator.IS_NOT_EMPTY:
                return ~(column.isna() | (column.astype(str).str.strip() == ''))
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œç¬¦: {operator}")
                
        except Exception as e:
            self.logger.error(f"åº”ç”¨åŒ¹é…æ“ä½œç¬¦å¤±è´¥: {e}")
            return pd.Series([False] * len(column))
    
    def execute_multiple_mappings(self, mappings: List[DataMapping], 
                                 source_data: Dict[str, pd.DataFrame], 
                                 target_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """æ‰§è¡Œå¤šä¸ªæ•°æ®æ˜ å°„"""
        current_target_data = target_data.copy()
        
        for mapping in mappings:
            try:
                current_target_data = self.execute_mapping(mapping, source_data, current_target_data)
            except Exception as e:
                self.logger.error(f"æ‰§è¡Œæ˜ å°„ '{mapping.name}' å¤±è´¥: {e}")
                continue
        
        return current_target_data
    
    def validate_mapping(self, mapping: DataMapping, source_data: Dict[str, pd.DataFrame], 
                        target_data: Dict[str, pd.DataFrame]) -> List[str]:
        """éªŒè¯æ•°æ®æ˜ å°„é…ç½®"""
        errors = []
        
        # æ£€æŸ¥æºæ–‡ä»¶
        if mapping.source_file not in source_data:
            errors.append(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {mapping.source_file}")
        else:
            df = source_data[mapping.source_file]
            # æ£€æŸ¥æºåˆ—ç´¢å¼•
            try:
                match_col_index = self._get_column_index(mapping.source_match_coordinate, df)
                value_col_index = self._get_column_index(mapping.source_value_coordinate, df)
                
                if match_col_index >= len(df.columns):
                    errors.append(f"æºåŒ¹é…åˆ—ç´¢å¼•è¶…å‡ºèŒƒå›´: {mapping.source_match_coordinate}")
                if value_col_index >= len(df.columns):
                    errors.append(f"æºå–å€¼åˆ—ç´¢å¼•è¶…å‡ºèŒƒå›´: {mapping.source_value_coordinate}")
            except Exception as e:
                errors.append(f"æºåˆ—é…ç½®é”™è¯¯: {e}")
        
        # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶
        if mapping.target_file not in target_data:
            errors.append(f"ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {mapping.target_file}")
        else:
            df = target_data[mapping.target_file]
            # æ£€æŸ¥ç›®æ ‡åˆ—ç´¢å¼•
            try:
                match_col_index = self._get_column_index(mapping.target_match_coordinate, df)
                insert_col_index = self._get_column_index(mapping.target_insert_coordinate, df)
                
                if match_col_index >= len(df.columns):
                    errors.append(f"ç›®æ ‡åŒ¹é…åˆ—ç´¢å¼•è¶…å‡ºèŒƒå›´: {mapping.target_match_coordinate}")
                if insert_col_index >= len(df.columns):
                    errors.append(f"ç›®æ ‡æ’å…¥åˆ—ç´¢å¼•è¶…å‡ºèŒƒå›´: {mapping.target_insert_coordinate}")
            except Exception as e:
                errors.append(f"ç›®æ ‡åˆ—é…ç½®é”™è¯¯: {e}")
        
        return errors 